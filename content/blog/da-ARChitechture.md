---
title: "Architecture D&A like a Pro"
date: 2025-04-13T00:20:21+01:00
draft: false
tags: ["Dev"]
description: 'From Data Modelling, through D&A Tech, to successfull Data Product Delivery.'
url: 'data-analytics-architecture'
---


You might encounter this kind of architectures:


```mermaid

graph LR
    subgraph Landing & Bronze
        direction LR
        L1 --> B1
        L2 --> B2
        L3 --> B3
    end

    subgraph Silver
        direction LR
        B1 --> S1
        B2 --> S2
        B3 --> S2
    end

    subgraph Gold
        direction LR
        S1 --> G1
        S2 --> G1
    end

```

I love [diagrams](https://www.mermaidchart.com/play#pako:eNp90N9LwzAQB_B_5ciDKKwPzrc-qOsPZA9OwQ6UZYysuW5h7UWStAPL_neTlskQ8SUP37vPJbmelVoii1kURZxKTZXaxZwAaq0PMewFycyII4XI7bHBGCRWoq0dp4FUtT6We2EcFFloApitOHtzIcmM6tBwtoYouoek5yx_z9NlMX9ZbLJZkYMiQOoeODuNNPGN8IF26E_9nCd08MtURjdB-bEXaKEHk_1jhNl1q-n6x6WDyL2Yv2aJsPiMzqjSXo9bmIBpaeNUgxvrzM3IRkm3HhVoHUjhxNajgz3_kqa-tiSnXI3SP0tiqFwB3f2Rn8c99qD8rTFwJr5ag_FwRpfTJ1Bp04QOo1uSKEP0qW1ItsMC2ekbNbOQkA)

```mermaid
---
config:
  look: handDrawn
  theme: default
---
flowchart TD
    A["Start Driver"] --> B{"EXECUTION_DATE in env?"}
    B -- Yes --> C["Get EXECUTION_DATE from env"]
    B -- No --> D["Get EXECUTION_DATE from argv[2]"]
    C --> E["IPDBaseMetrics(config, run_time_str)"]
 
    n1["Test databricks"] --> n2["Untitled Node"] & n3["Untitled Node"]
    n1@{ icon: "azure:azure-databricks", form: "rounded", pos: "b"}
```


1. An Operational Data Hub (ODH) is a central, integrated data store that serves operational systems and analytical applications with near real-time or real-time data.

It acts as a single source of truth for operational data, consolidating information from various source systems.

{{< details title="Key characteristics of an ODH üìå" closed="true" >}}

* Real-time or near real-time data ingestion and delivery.
* Data integration and transformation.
* Support for operational analytics and decision-making.
* Lower latency compared to traditional data warehouses.

{{< /details >}}



2. Data LakeHouses


{{< callout type="warning" >}}
Always make comprehensible **Designs & Docs** for all the parties involved in the project
{{< /callout >}}


### Data Modelling

Design a [Data Story](https://jalcocert.github.io/JAlcocerT/business-intelligence-data-analytics/#designing-an-analytical-flow)

https://jalcocert.github.io/JAlcocerT/data-basics-for-data-analytics/#others

* A **conceptual data model** is the highest level, and therefore the least detailed.
* A **logical data model** involves more detailed thinking about the implementation without actually implementing anything.
* Finally, the **physical data model** draws on the requirements from the logical data model to create a real database.

{{< cards cols="1" >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/sql-data-analytics/" title="SQL for D&A ‚Üó " >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/data-basics-for-data-analytics/" title="Data Modelling 101 ‚Üó" >}}
{{< /cards >}}

### Tech

{{< cards cols="1" >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/sql-data-analytics/" title="SQL for D&A ‚Üó " >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/data-basics-for-data-analytics/" title="DataBricks 101 ‚Üó" >}}
{{< /cards >}}

{{< cards cols="1" >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/sql-data-analytics/" title="GCP and BQ 101 ‚Üó " >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/data-basics-for-data-analytics/" title="PySpark 101 ‚Üó" >}}
{{< /cards >}}


```py
Test_Data = [("James","Sales","NY",90000,34,10000),
    ("Michael","Sales","NY",86000,56,20000),
    ("Robert","Sales","CA",81000,30,23000),
    ("Maria","Finance","CA",90000,24,23000),
    ("Raman","Finance","CA",99000,40,24000),
    ("Scott","Finance","NY",83000,36,19000),
    ("Jen","Finance","NY",79000,53,15000),
    ("Jeff","Marketing","CA",80000,25,18000),
    ("Kumar","Marketing","NY",91000,50,21000)
  ]

schema = ["employee_name","department","state","salary","age","bonus"]
Test_DF = spark.createDataFrame(data=Test_Data, schema = schema)
Test_DF.printSchema()
Test_DF.show(truncate=False)
```

---



## From the Experience

Whatever the industry you are working on, make sure to set and improve your [workflow for effectiveness](https://jalcocert.github.io/JAlcocerT/telecom-concepts-101/#workflow-for-effectiveness):

1. What's going on
2. Meeting Scheduler Template
3. RCA Template
4. MTG Summary Template

{{< callout type="info" >}}
Understand *the one thing* the client value the most. [Pareto](https://jalcocert.github.io/JAlcocerT/product-skills-for-data-analytics/)?
{{< /callout >}}

### PM Skills

{{< cards cols="2" >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/business-analytics-skills/#raci-matrix" title="RACI 101 ‚Üó " >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/setup-bi-tools-docker/#conclusions" title="A Project Charter Handy ‚Üó " >}}  
{{< /cards >}}

Get ready to organize effective meetings:

{{< cards cols="2" >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/business-analytics-skills/#facilitating-meetings" title="BA MTG Facilitating Skills ‚Üó " >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/effective-meetings-data-analytics/#as-a-meeting-owner" title="Effective Meeting Owership ‚Üó " >}}  
{{< /cards >}}


https://jalcocert.github.io/JAlcocerT/business-analytics-skills/#prototyping-mockups-and-wireframes

### BA & Elicitation Skills

To tell better stories with data, you have to [ask the right questions](https://jalcocert.github.io/JAlcocerT/business-analytics-concepts/#business-analysis-key-questions):


* What are the kinds of changes we are doing?
* **What are the needs we are trying to satisfy?**
* Who are the stakeholders involved?
* **What do stakeholders consider to be of value?**

{{< callout type="info" >}}
For more questions, you can switch on your **[product](https://jalcocert.github.io/JAlcocerT/product-skills-for-data-analytics/#faq) mindset**
{{< /callout >}}

<!-- ![Cat product Meme](/blog_img/memes/features-vs-needs.png) -->

{{< cards >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/product-skills-for-data-analytics/#faq" title="Product MindSet" image="/blog_img/memes/features-vs-needs.png" subtitle="Questions to Define Products" >}}
{{< /cards >}}


{{< details title="My Favourite Questions for Requirement Gathering üìå" closed="true" >}}



{{< /details >}}


<!-- 
referencia a persuasion escrito en jan 2022

y aqui cositas de neuromarketing

the buying brain....
`brainfluence`

 -->


> These are very helpful on end to end projects - where we went from raw data modelling to [BI solutions](https://jalcocert.github.io/JAlcocerT/setup-bi-tools-docker/#syncing-expectations):

You can also have handy a list of QQ to enable others perform their job:

https://jalcocert.github.io/JAlcocerT/team-management-data-analytics/#enabling-others

---

## Conclusions

{{< cards >}}
  {{< card link="https://jalcocert.github.io/JAlcocerT/get-started-with-flask/" title="Flask Intro" image="/blog_img/apps/flask-nginx-duckdns.png" subtitle="Deployed a Flask WebApp with https and NGINX to Hertzner" >}}
  {{< card link="https://github.com/JAlcocerT/flask_sensor_display" title="Flask Sensor Display" image="/blog_img/outro/telecom/cm-fiber-coax.png" subtitle="Source Code on Github" >}}
{{< /cards >}}





---

## FAQ

### Example

> tienes una fabrica de leche y quieres extraer tres arhcivos: lsita de vacas, medidas de leche y otro de calidad de la leche. Quieres crear facts y dimensions: como lo harias?

¬°Claro! Vamos a estructurar tus datos de la f√°brica de leche en un modelo de datos dimensional, separando hechos (facts) y dimensiones (dimensions). Esto te permitir√° analizar la informaci√≥n de manera eficiente y obtener insights valiosos.

**1. Identificaci√≥n de Hechos (Facts)**

Los hechos son las mediciones num√©ricas que quieres analizar. En tu caso, los principales hechos son:

* **Medidas de Leche:** La cantidad de leche producida por cada vaca en un per√≠odo de tiempo.
* **Calidad de la Leche:** Los indicadores de calidad de la leche (por ejemplo, contenido de grasa, prote√≠nas, recuento de c√©lulas som√°ticas).

**2. Identificaci√≥n de Dimensiones (Dimensions)**

Las dimensiones son los atributos descriptivos que contextualizan los hechos. En tu caso, las dimensiones clave son:

* **Vacas:** Informaci√≥n sobre cada vaca (ID, raza, edad, etc.).
* **Tiempo:** Fechas y horas de las mediciones (d√≠a, semana, mes, a√±o).
* **Calidad:** Los tipos de calidad de la leche (por ejemplo, contenido de grasa, prote√≠nas, recuento de c√©lulas som√°ticas).

**3. Dise√±o del Modelo Dimensional**

Podemos crear un esquema de estrella, que es com√∫n para el modelado de datos dimensionales. Tendremos una tabla de hechos central y tablas de dimensiones que la rodean.

**Tabla de Hechos: Producci√≥n de Leche**

* Esta tabla contendr√° los hechos principales:
    * `ID_Vaca` (clave for√°nea a la tabla de dimensi√≥n "Vacas")
    * `ID_Tiempo` (clave for√°nea a la tabla de dimensi√≥n "Tiempo")
    * `ID_Calidad` (clave for√°nea a la tabla de dimensi√≥n "Calidad")
    * `Cantidad_Leche` (la cantidad de leche producida)
    * `Valor_Calidad` (el valor de la calidad de la leche)

**Tablas de Dimensiones:**

* **Vacas:**
    * `ID_Vaca` (clave primaria)
    * `Raza`
    * `Edad`
    * `Fecha_Nacimiento`
    * Otros atributos relevantes de las vacas
* **Tiempo:**
    * `ID_Tiempo` (clave primaria)
    * `Fecha`
    * `Hora`
    * `Dia_Semana`
    * `Mes`
    * `A√±o`
* **Calidad:**
    * `ID_Calidad` (clave primaria)
    * `Tipo_Calidad` (por ejemplo, "Grasa", "Prote√≠na", "C√©lulas Som√°ticas")
    * `Unidad_Medida` (por ejemplo, "porcentaje", "unidades/mL")

**4. Procesamiento de Datos (ETL)**

Necesitar√°s un proceso ETL (Extracci√≥n, Transformaci√≥n, Carga) para mover los datos desde tus archivos originales a este modelo dimensional.

* **Extracci√≥n:** Lee los datos de tus archivos CSV (lista de vacas, medidas de leche, calidad de la leche).
* **Transformaci√≥n:** Limpia y transforma los datos seg√∫n sea necesario. Esto puede incluir:
    * Estandarizar formatos de fecha y hora.
    * Manejar valores faltantes.
    * Convertir unidades de medida.
    * Crear claves primarias y for√°neas.
* **Carga:** Carga los datos transformados en las tablas de hechos y dimensiones de tu almac√©n de datos.

**5. An√°lisis y Reportes**

Con este modelo dimensional, podr√°s realizar an√°lisis como:

* Producci√≥n de leche por vaca a lo largo del tiempo.
* Calidad de la leche por raza de vaca.
* Tendencias de producci√≥n de leche por mes o a√±o.
* Comparaci√≥n de la calidad de la leche entre diferentes vacas.

Espero que esta estructura te sea de utilidad.

> en bronze layer podriamos tener los datos cargados de API, que podriamos tener en silver y luego en gold?

¬°Excelente pregunta! La arquitectura Medallion (Bronce, Plata, Oro) es una forma muy efectiva de organizar y procesar datos en un data lakehouse. Aqu√≠ te explico c√≥mo podr√≠as estructurar tus datos de la f√°brica de leche en cada capa:

**Capa Bronce (Bronze Layer): Datos Crudos de la API**

* **Prop√≥sito**:
    * Esta capa almacena los datos tal cual los recibes de la API, sin ninguna transformaci√≥n.
    * Su objetivo es preservar la integridad y el linaje de los datos originales.
    * Permite la recuperaci√≥n de datos hist√≥ricos y la re-ejecuci√≥n de procesos si es necesario.
* **Contenido**:
    * Datos JSON o XML directamente de la API.
    * Registros de eventos de la API.
    * Archivos CSV o Parquet sin procesar.
    * En tu caso, aqu√≠ llegar√≠an los datos de la API relacionados con:
        * Lecturas de sensores de producci√≥n de leche.
        * Resultados de an√°lisis de calidad de leche.
        * Datos de identificaci√≥n de vacas.
        * Informaci√≥n meteorol√≥gica que pueda afectar la producci√≥n.
* **Formato**:
    * Se recomienda usar formatos que conserven la estructura original de los datos, como JSON, Avro o Parquet.

**Capa Plata (Silver Layer): Datos Limpios y Estandarizados**

* **Prop√≥sito**:
    * Esta capa se enfoca en la limpieza, estandarizaci√≥n y validaci√≥n de los datos.
    * Se aplican transformaciones b√°sicas para asegurar la calidad y consistencia de los datos.
    * Se crea una versi√≥n "curada" de los datos, lista para an√°lisis posteriores.
* **Contenido**:
    * Datos limpios y validados.
    * Tablas con esquemas definidos.
    * Datos estandarizados en formatos consistentes.
    * En tu caso, aqu√≠ tendr√≠as:
        * Tablas de "vacas" con datos consistentes y validados.
        * Tablas de "mediciones de leche" con fechas y horas estandarizadas, y valores num√©ricos limpios.
        * Tablas de "calidad de la leche" con unidades de medida consistentes y valores validados.
        * Tablas de informacion del clima, con datos limpios y validados.
* **Formato**:
    * Formatos eficientes para el almacenamiento y la consulta, como Parquet o Delta Lake.

**Capa Oro (Gold Layer): Datos Agregados y Optimizados para el An√°lisis**

* **Prop√≥sito**:
    * Esta capa contiene datos altamente refinados, agregados y optimizados para casos de uso espec√≠ficos.
    * Se crean modelos dimensionales (como el que discutimos anteriormente) para facilitar el an√°lisis y la generaci√≥n de informes.
    * Los datos est√°n listos para ser consumidos por herramientas de BI y an√°lisis.
* **Contenido**:
    * Tablas de hechos y dimensiones.
    * Agregaciones y res√∫menes de datos.
    * Modelos de datos espec√≠ficos para informes y paneles.
    * En tu caso, aqu√≠ tendr√≠as:
        * Tablas de hechos de "producci√≥n de leche" y "calidad de la leche".
        * Tablas de dimensiones de "vacas", "tiempo" y "calidad".
        * Vistas y tablas agregadas para an√°lisis de tendencias, rendimiento de vacas, etc.
* **Formato**:
    * Formatos optimizados para consultas r√°pidas, como Parquet o Delta Lake, organizados de manera eficiente.

**Resumen de beneficios**

* **Calidad de datos mejorada**: A medida que los datos avanzan por las capas, se someten a procesos de limpieza y validaci√≥n.
* **Linaje de datos claro**: Cada capa preserva el historial de los datos, lo que facilita la auditor√≠a y la resoluci√≥n de problemas.
* **Eficiencia en el an√°lisis**: La capa Oro proporciona datos optimizados para consultas y an√°lisis, lo que acelera la toma de decisiones.
* **Flexibilidad**: La arquitectura Medallion se adapta a una variedad de casos de uso y herramientas de an√°lisis.

Siguiendo esta estructura, podr√°s construir un data lakehouse robusto y eficiente para tu f√°brica de leche.


> en la capa oro seria donde definimos nuestros facts y dimensions?

S√≠, exactamente. La capa Oro es donde se definen y materializan los modelos dimensionales, incluyendo las tablas de hechos y dimensiones. Aqu√≠ te explico por qu√©:

**Capa Oro: El Destino para el An√°lisis**

* **Optimizaci√≥n para el consumo:**
    * La capa Oro est√° dise√±ada espec√≠ficamente para el consumo de datos por parte de herramientas de BI, an√°lisis y generaci√≥n de informes.
    * Los datos se transforman y estructuran de manera que sean f√°ciles de entender y consultar para los usuarios finales.
* **Modelado dimensional:**
    * El modelado dimensional, con sus tablas de hechos y dimensiones, es una t√©cnica com√∫n para organizar datos para el an√°lisis.
    * Este modelo facilita la realizaci√≥n de consultas complejas y la generaci√≥n de informes significativos.
* **Agregaciones y res√∫menes:**
    * En la capa Oro, los datos se agregan y resumen para proporcionar informaci√≥n de alto nivel.
    * Esto reduce la necesidad de realizar c√°lculos complejos en tiempo real, lo que mejora el rendimiento de las consultas.

**Por qu√© la capa Oro es ideal para hechos y dimensiones**

* **Claridad y estructura:**
    * Las tablas de hechos y dimensiones proporcionan una estructura clara y organizada para los datos, lo que facilita su comprensi√≥n.
    * Esto es esencial para los usuarios de negocios que necesitan acceder y analizar los datos.
* **Rendimiento de las consultas:**
    * El modelado dimensional est√° optimizado para consultas anal√≠ticas, lo que permite obtener resultados r√°pidos y eficientes.
    * Esto es fundamental para las herramientas de BI que necesitan interactuar con los datos en tiempo real.
* **Facilidad de uso:**
    * La estructura de hechos y dimensiones simplifica la creaci√≥n de informes y paneles, lo que permite a los usuarios explorar los datos de manera intuitiva.
    * Esto permite mayor facilidad a los usuarios finales, para la creaci√≥n de reportes.

**En resumen, la capa Oro es el lugar donde los datos se transforman en informaci√≥n valiosa y f√°cil de usar para la toma de decisiones.**

### More T-Shaped Skills
